#### Cose da fare ####

-fare memory mapping del disco
- mapdb
- extralarge mapping! Non servono blocchi!

Fabiano:

    Create test collection.

    Create package.

    Document processing:
      X INPUT: data collection as a list of: <docno>\t<text>\n
      X OUTPUT: data collection processed in the format: list of <docno>\t<term1 term2 term3...termN>\n
          X -> You must read the data using UNICODE instead of basic ASCII.
            -> Deal with raw form with various errors (empty page, malformed lines, malformed characters).
          X -> Remove the punctuation signs.
          X -> Stemming e stopword removal (OPTIONAL).

Ema:
    Inverted index:
        INPUT: list of <docno>\t<term1 term2 term3...termN>\n
        OUTPUT: files containing the inverted index data structures (INVERTED INDEX, LEXICON, DOCUMENT TABLE)
            -> Parsing docs (Maybe it's the document processiong part(?))
            -> Generate intermediate posting lists and writing unsorted or partially sorted
            -> Sort or merge the posting
            -> Reformat the sorted posting into the final structure

We must compute not a single document, but blocks of documents, in particular we must read N lines from the collection,
these N lines forms a block, so we will have (# OF DOCS)\N blocks, for each block we must create:
- A dictionary with: K = term, V = term_id
- An inverted index
-- It must not be sorted, just accumulation of posting in the posting lists (The merge phase will do it)
-- The posting must contain: term_id\tposting1 posting2 ... postingN where posting = doc_id,frequency
-- One file must contain the doc_id list and another the respective freq
- Document index, while we process the document, we must compute the size of the document

-------- SPIMI ALGORITHM ------

SPIMI-INVERT(token_stream) token_stream is obtained during the parsing of the document
    output_file = NEWFILE()
    dictionary = NEWHASH()
    while(free_memory_available)
    do token <- next(token_stream)
        if term(token) not in dictionary
            then posting_list = ADDTODICTIONARY(dictionary, term(token))
        else posting_list = GETPOSTINGSLIST(dictionary, term(token))
        if full(posting_list)
            then posting_list = DOUBLEPOSTINGSLIST(dictionary, term(token))
        ADDTOPOSTINGLIST(posting_list, docID(token))
    sorted_terms <- SORTTERMS(dictionary)
    WRITEBLOCKTODISK(sorted_terms, dictionary, output file)


lista
    doc id
    lista
        parole

lista
    (doc_id, parola)

MERGE ALL BLOCKS


-docno e docid
-dimensione dei blocchi
-merging
-modularità
-lettura compressa
-flag





1 a b c d e
2 a g y e w
3 e f h u l

                a b c d e g y w f h l

        ->      a 1 2 ---> 1 : 0
                b 1   ---> 1 : 3
                c     ---> 2 : 4

                a 1 2




                b 1






Scelta HashMap per la costruizione iniziale rispetto a TreeMap:
- Il motivo è che l'inserimento con HM ha complessità O(1), analogamente con le get O(1), di conseguenza l'aggiornamento,
  la lettura e l'inserimento di informazioni aggiuntive su un singolo term hanno tutte complessità O(1). -> O(n)
  Usando TM sarebbero O(logn) n è il numero di termini inseriti fino a quel momento -> O(nlogn)
  Questo vale per la costruzione del lexicon
  Per l'ordinamento abbiamo deciso di costruire un TM a partire da una HM in modo tale da ordinare (senza doppioni)
  durante la costruzione con complessita O(logm) dove m è |lexicon| cardinalità lexicon. -> O(mlogm)
  Questo è meglio in quanto n > m, perchè in n consideriamo ogni singolo termine, i doppioni, l'aggiornamento delle
  informazioni, mentre in m abbiamo solo i termini già aggiornati e senza doppioni. dunque n > m.

Scelta algoritmo di sorting:
- inserimento ordinato in una tree map


readFully(byte[] b, int off, int len)
Reads exactly len bytes from this file into the byte array, starting at the current file pointer.

Come aggiungere l'offset? fare un for normale finche non troviamo l'id giusto? Scritto una volta non si tocca più.

Aggiungere il numero di conteggi della parola nel lexicon ad ogni occorrenza

Domani: Aggiungere scrittura del document index