#### Cose da fare ####

Fabiano:

    Create test collection.

    Create package.

    Document processing:
      X INPUT: data collection as a list of: <docno>\t<text>\n
      X OUTPUT: data collection processed in the format: list of <docno>\t<term1 term2 term3...termN>\n
          X -> You must read the data using UNICODE instead of basic ASCII.
            -> Deal with raw form with various errors (empty page, malformed lines, malformed characters).
          X -> Remove the punctuation signs.
          X -> Stemming e stopword removal (OPTIONAL).

Ema:
    Inverted index:
        INPUT: list of <docno>\t<term1 term2 term3...termN>\n
        OUTPUT: files containing the inverted index data structures (INVERTED INDEX, LEXICON, DOCUMENT TABLE)
            -> Parsing docs (Maybe it's the document processiong part(?))
            -> Generate intermediate posting lists and writing unsorted or partially sorted
            -> Sort or merge the posting
            -> Reformat the sorted posting into the final structure

We must compute not a single document, but blocks of documents, in particular we must read N lines from the collection,
these N lines forms a block, so we will have (# OF DOCS)\N blocks, for each block we must create:
- A dictionary with: K = term, V = term_id
- An inverted index
-- It must not be sorted, just accumulation of posting in the posting lists (The merge phase will do it)
-- The posting must contain: term_id\tposting1 posting2 ... postingN where posting = doc_id,frequency
-- One file must contain the doc_id list and another the respective freq
- Document index, while we process the document, we must compute the size of the document

-------- SPIMI ALGORITHM ------

SPIMI-INVERT(token_stream) token_stream is obtained during the parsing of the document
    output_file = NEWFILE()
    dictionary = NEWHASH()
    while(free_memory_available)
    do token <- next(token_stream)
        if term(token) not in dictionary
            then posting_list = ADDTODICTIONARY(dictionary, term(token))
        else posting_list = GETPOSTINGSLIST(dictionary, term(token))
        if full(posting_list)
            then posting_list = DOUBLEPOSTINGSLIST(dictionary, term(token))
        ADDTOPOSTINGLIST(posting_list, docID(token))
    sorted_terms <- SORTTERMS(dictionary)
    WRITEBLOCKTODISK(sorted_terms, dictionary, output file)

MERGE ALL BLOCKS


0 SBA NSA KSA IOE
1 MDOQW MO MO O
2 ABC DA DAS Q
3 DOMASK  DSA ABC
4 ABC DASJK KK
5 KMD ABC KDAS

LEXICON:
SBA 1
NSA 2
KSA 3
IOE 4
ABC 5


FILE BLOCK A
1   0
2   0
5   2 3 4 5

BLOCK B

6 7 8 9 10

LEXICON B:
ABC 3
FILE BLOCK B

3 8 9 10

2 3 4 5 8 9 10
























